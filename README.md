# AerogelML_301381
Aerogel Machine Learning group project, Group ID 26
[301381] Giovanni Lolli, [296221] Umberto Cipriani, [295781] Peter Luigi Festa

AI project
We started by getting the data ready for analysis. First, we loaded the dataset and checked for any missing values. For numbers, we filled in the missing values with the middle value (median), and for categories, we used the most common value. This made sure the data was complete and ready to use.
Next, we explored the data with some visualizations. We created a histogram to see the age range of applicants and how often each age appeared. We also used a boxplot to compare bonding risk ratings across different job statuses. Lastly, we made a scatter plot to see if there was a relationship between the amount of material processed and the bonding risk ratings. These graphs helped us understand the data better.
After cleaning the data, we prepared it for building a model. We chose” BondingSuccessful “as the target variable to predict and used the other columns as features. Since some features were categories, we turned them into numbers using one-hot encoding. We then split the data into training and testing sets, with 70% for training and 30% for testing. Finally, we scaled the data so that all features were on the same scale, which is important for the algorithms we were going to run
The 3 algorithms that we choose were:
1) Logistic regression
2) Cart trees
3) Random forest
Logistic Regression
We trained a logistic regression model using the training data and then tested it with the test data. We checked how accurate the model was and created a confusion matrix to see how often it got things right or wrong. We also calculated precision, recall, and F1-scores to see how well the model balanced its predictions.
To understand how good the model was at separating successful and unsuccessful bonding, we looked at the ROC curve and calculated the Area Under the Curve (AUC). This showed us how well the model worked at different thresholds. We also looked at the Precision-Recall curve and calculated the Average Precision (AP) score, which is helpful for imbalanced datasets.
Finally, we analyzed which features were most important for the model’s predictions. The logistic regression model gives coefficients that show how much each feature matters. We turned this into a bar chart to clearly see which features had the biggest impact on bonding success.
Random forest
TRAINING AND VALIDATION: In this algorithm we trained the random forest model and evaluated on the validation set First we split the data into training and validation sets, then we trained the random forest model with default parameters on the training set validation set evaluates the model , printing out metrics such as precision accuracy recall f1 score and the confusion matrix and lastly plotting the roc-auc curve The training results in a 95% accuracy, a weighted F1 score of 95% as well and a roc auc of 0.99 Recall for positive class could improve as 21% of positives were missed TEST SET: Testing the performance of the random forest model evaluating the performance on the test set Printing metrics as precision recall f1 score and confusion matrix and displaying the roc-auc curve The untuned model performs well but tuning would most likely improves metrics like recall and F1-score It's important to notice the consistency with validation set indicating robustness
TUNING SET: In this algorithm we performed hyperparameter tuning for the Random forest classifier First of all , we loaded the dataset removing missing values and processing categorical features thanks to one hot encoding Then we went over the dataset splitting into training and validation sets in a stratified measure to ensure balance To perform the tuning we were actually in doubt in choosing between GridsearchCV and RandomizedSearchCV. ​​​​​​​                     We proceeded to pick Gridsearch as the dataset isn't too large and we wanted to focus on accuracy rather than timing Wedecided to use GridSearchCV with a parameter grid (n_estimators,max_depth,min_samples_split,class_weight) to ding the optimal parameters for the random forest model. Scoring metric is f1-score that manages to balance precision and recall So basically everything boils down to identifying the best parameters to improve even more the performance of the model
Cart trees
Training set . We started by preparing the data for our Decision Tree Classifier. Rows with missing values in the target column (BondingSuccessful) were removed to ensure the model was trained only on complete data. We then split the cleaned data into three sets: 70% for training, 20% for validation, and 10% for testing. This setup allowed us to train the model, fine-tune it on the validation set, and evaluate its final performance on the test set.
Next, we trained the Decision Tree Classifier on the training data using predefined parameters. Once the model was trained, we used it to generate predictions for the validation set. This allowed us to assess how well the model performed before testing it on the unseen test data. We evaluated the model’s performance using several metrics. A classification report was generated, which included precision, recall, F1-score, and accuracy, to give us a detailed view of how well the model predicted each class. We also created a confusion matrix to see how often the model made correct and incorrect predictions for each category. To understand the trade-off between sensitivity and specificity, we calculated the ROC-AUC score and plotted the ROC curve to visualize the model’s classification performance across different thresholds.
Finally, we visualized the structure of the trained Decision Tree. This helped us interpret the model’s decision-making process by showing the splits it made at each level and the conditions it used to classify the data. This combination of metrics, visualizations, and insights allowed us to thoroughly evaluate the model and understand how it arrived at its predictions.
 
Tuning set: To find the best combination of hyperparameters for the Decision Tree, we used Grid Search with Cross-Validation. Using 5-fold cross-validation, we tested multiple hyperparameter combinations to determine which settings optimized the model’s performance. The F1 score was chosen as the metric for selecting the best model, ensuring a balance between precision and recall. Once the grid search was complete, we selected the model with the highest F1 score for further evaluation.
The tuned model was then applied to the validation set to assess its performance. We evaluated its predictions using several metrics, including precision, recall, F1-score, and accuracy, providing a comprehensive view of its effectiveness. A confusion matrix was generated to summarize the classification results, highlighting how well the model predicted each class. To better understand the model’s ability to distinguish between classes, we plotted the ROC-AUC curve, which illustrated its performance across various decision thresholds.
In the end, we identified the best hyperparameters for the Decision Tree, evaluated its tuned performance on the validation set, and visualized its classification results and discriminative power. This approach ensured that the model was both optimized and thoroughly evaluated before deployment.
 
